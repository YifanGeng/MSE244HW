{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Name(s) Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to MS&E 244: Statistical Arbitrage - Homework 2.\n",
    "\n",
    "This assignment is due by January 27th, 2026 at 11:59pm Pacific Time and should be done in groups of 2-3 people. We recommend collaborating via a private GitHub repo.\n",
    "\n",
    "Instructions for this assignment are below. **Please read all instructions carefully.**\n",
    "\n",
    "1. Replace \"Your Name(s) Here\" above in bold with your name, and, if working in a group, your group members' names.\n",
    "2. Read and run the instructions code in the Setup section. Make sure you understand what it does.\n",
    "3. Answer the questions in the sections below. Make sure to limit code output to a reasonable length so that the resulting PDF you'll make from this notebook is readable. If the resulting PDF is not readable, keep your lines of code less than 80 characters.\n",
    "4. Once finished, export or convert the notebook to PDF, using any method you like. For example, you can use the File -> Save and Export Notebook As -> PDF option in Jupyter Lab, you can use the [nbconvert](https://nbconvert.readthedocs.io/en/latest/) command line tool, etc. Whatever displays best is fine. Make sure no code is cut off in the PDF and that sheets are 8.5x11 inch dimensions or similar.\n",
    "5. Submit the resulting PDF to the Gradescope assignment. If working in a group, only one person needs to submit and select all group members on Gradescope.\n",
    "6. Zip and submit all files (your .ipynb file and any .py files we provided or you created) to the Canvas assignment. If working in a group, only one member of the group needs to submit to Canvas. Unlike Gradescope, no group members need to be selected on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup  {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**System Requirements**\n",
    "\n",
    "First, ensure you have the proper system requirements:\n",
    "\n",
    "1. We will only support Mac or Linux in this course. If you're on Windows, learn to use [WSL 2.0](https://learn.microsoft.com/en-us/windows/wsl/install) to run Linux on Windows, and then run these commands via WSL.\n",
    "2. Ensure you have Python 3.10 and pip installed. You also need something to run this Jupyter notebook, whether that's Jupyter Notebook, Jupyter Lab, an IDE like VS Code, or Google Colab. We recommend collaborating via GitHub and using Jupyter Lab or VS Code to complete this notebook. \n",
    "\n",
    "**Jupyter Lab**\n",
    "\n",
    "To use Jupyter Lab, we'll need to set up the Python virtual environment and install the requirements. We've provided example instructions for doing so below for those using Jupyter Lab. Note that you may need to substitute `pip` with `pip3` depending on your installation. You can learn more by reading the [venv docs](https://docs.python.org/3/library/venv.html) and the [ipykernel docs](https://ipython.readthedocs.io/en/stable/install/kernel_install.html). See documentation for [Jupyter Lab](https://jupyterlab.readthedocs.io/en/latest/) if needed.\n",
    "\n",
    "```bash\n",
    "# Navigate to the root directory containing your course files\n",
    "cd /path/to/mse244\n",
    "\n",
    "# Place the requirements.txt file in this directory if it isn't already there\n",
    "# Put this .ipynb file in this directory or in a subdirectory, e.g. \"hw1\"\n",
    "\n",
    "# Create a virtual environment\n",
    "python3.10 -m venv .venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "source .venv/bin/activate\n",
    "\n",
    "# Install the required packages\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Add the virtual environment to Jupyter\n",
    "python -m ipykernel install --user --name mse244 --display-name=\"MSE244\"\n",
    "\n",
    "# Start Jupyter Lab\n",
    "jupyter lab\n",
    "\n",
    "# Once in Jupyter, make sure to select the \"MSE244\" kernel \n",
    "```\n",
    "\n",
    "**VS Code**\n",
    "\n",
    "If using VS Code, read the docs for [Jupyter notebooks in VS Code](https://code.visualstudio.com/docs/datascience/jupyter-notebooks) and [virtual environments in VS Code](https://code.visualstudio.com/docs/python/environments). VS Code can help you create and activate a virtual environment and Jupyter kernel for this notebook through its GUI (though you should also learn how to do these things via the command line). Make sure that you use Python 3.10 to create your virtual environment.\n",
    "\n",
    "\n",
    "Once setup is complete and you're running within your environment, you can begin executing the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mpl_ticker\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# We add any local imports (Python files we've written) here\n",
    "from backtest import (\n",
    "    load_data,\n",
    "    plot_asset_with_max_return,\n",
    "    select_asset_universe,\n",
    "    form_pairs,\n",
    "    estimate_hedge_ratio,\n",
    "    compute_signal,\n",
    "    allocate_positions,\n",
    "    run_backtest,\n",
    ")\n",
    "\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 40)\n",
    "pd.set_option('display.precision', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we'll load the supplied Japanese equity data, preprocess the data, and set up the necessary variables.\n",
    "\n",
    "Please make sure to look up and learn about any lines you don't understand by reading the [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html). This is essential knowledge for anyone working with financial data in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'NIKKEI_CSV_PATH': 'N225.csv'\n",
    "}\n",
    "\n",
    "prices, returns, tickers, metadata = load_data(config, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look into some issues with the data. Some of you may have noticed in the last homework that there were a few strange returns or prices. Although we won't grade based on this, these were intentionally left in to sharpen your data cleaning skills. If you noticed them, congrats! Now we will explore these issues and learn about what to do with them.\n",
    "\n",
    "Below, we've written code to do just that. Take a look at the function called `plot_asset_with_max_return`, which is specified in the `backtest.py` file. Try running the function to plot the largest returns in the dataset. Study the output of the function (i.e. the plot) for `max_rank` in {0, 1, 2, 3} to see the different assets with the largest returns. (Note that we won't study or attempt to change anything past max_rank=3 for the purposes of this class--feel free to explore, though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_asset_with_max_return(returns, prices, max_rank=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some issues. Let's look into them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate 2269.T, which has the top rank (0). What's going on here? Use the `metadata` and the internet (generative AI ok here--but check the sources!) to find out what's going on. What do you think this price difference might be due to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's investigate 9201.T, which has the second and third-highest return. Again, use the metadata and the internet to find out what the price difference (and delisting) might be due to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's investigate 7974.T, which has the third-highest return. Again, find out what might've caused this price difference. Is it different from the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectively, the best way to deal with erroneous data is to correct it. However, we don't have the ability to do that. The data might have come from an old vendor, a non-responsive government agency, or a human who made errors they weren't aware of (and, they're off on vacation until next month). In these cases, we have a few options: filtering the data (effectively removing the missing data from consideration), imputing the missing values, or using a different data source altogether. Although imputation can be useful for features or signals, it can be a bad idea for asset prices, as prices impact your profit and loss calculations. We'll instead explore the first option: filtering the data. \n",
    "\n",
    "In this case, we filter the assets. This is known as *asset universe selection*. This is the process of selecting a subset of assets to include for consideration in your analysis, model, or trading. You don't *have* to analyze, trade, etc. these assets, but they'll be in the *universe* of assets that you may want to analyze, model, or trade. Typical asset filters may include minimum number of observations, minimum market cap, minimum liquidity, minimum volume, maximum absolute return, maximum volatility, etc. Here, since we've already restricted ourselves to the large, liquid stocks of the Nikkei index, we don't need those filters. Instead, we'll develop a filter based on data availability and validity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `backtest.py` contains a modified implementation of the Gatev et al. pairs trading strategy. In this HW, we'll finish the implementation and test it. (N.B.: `backtest.py` contains a lot of code. Most of it you don't need to touch, though it's worth reading if you have extra time, as it's a good reference for the code you're writing, and it may be useful for the project.)\n",
    "\n",
    "In this question, we'll complete one function in it: `select_asset_universe`. In this function, you'll filter the given prices and returns dataframes based on a filter. Your filter should do the following:\n",
    "\n",
    "1. First, ensure the lookback period is correct. It should be of size `lookback_period` and include dates up to, but not including, the passed `date`.\n",
    "2. Filter out stocks with any missing prices in the lookback period.\n",
    "3. Filter out stocks with absolute returns > `max_abs_return` in the lookback period.\n",
    "4. Return the filtered prices and returns dataframes, along with the tickers of the filtered stocks.\n",
    "\n",
    "Our implementation is about 12 lines, but it may be done in more or fewer lines.\n",
    "\n",
    "Complete the function in `backtest.py` and paste the completed `select_asset_universe` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test it. Execute this cell, which runs your asset filter in a loop and displays your asset filter's selection over time as a boolean matrix. Try different values for `lookback_period` and `max_abs_return` to see how they affect the selection, but finish by running it with `lookback_period=252` and `max_abs_return=0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'FILTER_MAX_ABS_RETURN': 0.5,\n",
    "    'FORMATION_PERIOD': 252,\n",
    "}\n",
    "\n",
    "asset_filter = pd.DataFrame(index=prices.index, columns=tickers, data=False)\n",
    "\n",
    "for date in prices.index[config['FORMATION_PERIOD']:]:\n",
    "    selected_stocks, selected_rets, selected_uni = select_asset_universe(\n",
    "        prices, \n",
    "        returns, \n",
    "        date, \n",
    "        config=config\n",
    "    )\n",
    "    asset_filter.loc[date, selected_stocks.columns] = True\n",
    "\n",
    "plt.imshow(asset_filter, aspect='auto', cmap='viridis', interpolation=None)\n",
    "plt.xlabel('Stock')\n",
    "plt.ylabel('Date')\n",
    "plt.title('Asset Filter Selection Over Time')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check to make sure it works properly to avoid selecting stocks with large returns. We'll check the stocks from Question 1 with the largest absolute returns during the lookback period. Run the cell below to see the results.\n",
    "\n",
    "You should not expect your filter to be perfect, but it should help avoid most of the time associated with the erroneous prices. In a real-world setting, we would want to use more advanced methods to filter out stocks that are not suitable for trading (as well as acquire good data, and avoid any lookahead bias). But for the purposes of the backtests we'll perform in this class, a filter like this suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionable_tickers = [\n",
    "    ('2269.T', '2009', '2010'),\n",
    "    ('9201.T', '2009', '2010'),\n",
    "    ('7974.T', '2007', '2017'),\n",
    "]\n",
    "\n",
    "# plot price of each stock and filter status for each stock\n",
    "for ticker, start, end in questionable_tickers:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax1 = plt.gca()\n",
    "    prices[ticker][start:end].plot(label='Price', color='blue', ax=ax1)\n",
    "    ax2 = ax1.twinx()\n",
    "    (1 * asset_filter[ticker][start:end]).plot(label='Filter', color='red', ax=ax2)\n",
    "    ax2.set_yticks([0, 1])\n",
    "    ax2.set_yticklabels(['Removed', 'Selected'])\n",
    "    plt.title(f'Price of {ticker}')\n",
    "    ax1.set_ylabel('Price', color='blue')\n",
    "    ax2.set_ylabel('Filter', color='red')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll continue on with the implementation of the GGR pairs trading strategy. We need to write a function to compute the distances between the stocks from the GGR paper. Your function should do the following:\n",
    "\n",
    "1. Take in a dictionary with the configuration parameter `NUM_PAIRS`, which is set to an integer such as 20. \n",
    "2. Normalize the price series of each stock to start at 1.\n",
    "3. If the `config['DISTANCE_METRIC']` configuration parameter is set to `\"ssd\"`, compute the sum of squared differences (SSD) between the normalized price series of each pair of stocks. Otherwise throw an error.\n",
    "4. Return a dataframe containing the top stock pairs and their corresponding distances.\n",
    "\n",
    "Feel free to locate the relevant part of the paper if you need more details. \n",
    "\n",
    "For reference, the return of the `form_pairs` function should look something like this:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\hline\n",
    "\\textbf{stock1} & \\textbf{stock2} & \\textbf{distance} \\\\\n",
    "\\hline\n",
    "A & B & 0.12 \\\\\n",
    "C & D & 0.23 \\\\\n",
    "E & F & 0.40 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For extra credit (0.5 points): in addition to the SSD metric, implement your own distance metric which is different from the SSD metric and produces a different ranking. Choose which metric to use based on the `DISTANCE_METRIC` configuration parameter of the `config` argument, which you can set to a string such as `\"ssd\"` or `\"my_metric\"`.\n",
    "\n",
    "Our implementation is fairly naive and constitutes about 16 lines. You can do it in fewer or more lines.\n",
    "\n",
    "Complete the function in `backtest.py` and paste the complete `form_pairs` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test it. Make sure your `form_pairs` function is working correctly by running the code below. We'll compute 5 pairs as a demo. This cell plots the top pair's prices over the lookback window. The prices should look pretty close and cross fairly frequently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a date for the asset filter\n",
    "date = pd.Timestamp('2010-01-04')\n",
    "config = {\n",
    "    'FILTER_MAX_ABS_RETURN': 0.5,\n",
    "    'FORMATION_PERIOD': 252,\n",
    "    'NUM_PAIRS': 5,\n",
    "    'DISTANCE_METRIC': 'ssd',\n",
    "}\n",
    "\n",
    "# Filter assets\n",
    "filtered_prices, filtered_returns, valid_stocks = select_asset_universe(\n",
    "    prices, returns, date, config=config\n",
    ")\n",
    "\n",
    "# Compute distances\n",
    "pairs = form_pairs(filtered_prices, config)\n",
    "display(pairs)\n",
    "\n",
    "# Plot the top pair's normalized prices over the (approximate) lookback window\n",
    "ticker1 = pairs.loc[0, 'stock1']\n",
    "ticker2 = pairs.loc[0, 'stock2']\n",
    "\n",
    "s1 = filtered_prices[date - pd.Timedelta(days=365):date][ticker1]\n",
    "s2 = filtered_prices[date - pd.Timedelta(days=365):date][ticker2]\n",
    "s1 = s1 / s1.iloc[0]\n",
    "s2 = s2 / s2.iloc[0]\n",
    "plt.plot(s1, label=f'Stock 1: {ticker1}')\n",
    "plt.plot(s2, label=f'Stock 2: {ticker2}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the ability to create pairs. However, we'll want to do some calculation of hedge ratios and cointegration tests to make sure the pairs are actually cointegrated. Cointegration is not covered in Gatev et al., but may (or may not!) improve the strategy. In any case, we'll apply it here as a filter.\n",
    "\n",
    "We've written a function called `estimate_hedge_ratio` in `backtest.py`. We'll use that function to estimate the hedge ratios, or *betas*, of the pairs. We'd like you to complete it. Do the following:\n",
    "\n",
    "1. Go through the `pairs` dataframe and for each pair, extract the price series for the two stocks.\n",
    "2. Use a method to compute the hedge ratio between the two stocks. If `config['HEDGE_RATIO_METHOD'] == 'ols'`, use OLS regression with no intercept to compute the hedge ratio by regressing stock 2 on stock 1. If `config['HEDGE_RATIO_METHOD'] == 'unit'`, set the hedge ratio to 1. Otherwise, throw an error.\n",
    "3. Use the hedge ratio to compute the spread $s_t = p_2(t) - \\beta p_1(t)$. Store the $\\beta$'s.\n",
    "4. Perform an ADF test on the spread (the `arch.unitroot` package is useful).\n",
    "5. Store stock1, stock2, the hedge ratio, the distance, the ADF statistic, the ADF p-value, whether the pair is cointegrated at the `config['COINT_THRESHOLD']` significance level in the results dataframe.\n",
    "6. Filter the results dataframe to only include pairs with an ADF p-value less than or equal to the threshold.\n",
    "7. Return the results dataframe.\n",
    "\n",
    "For reference, the return of the `estimate_hedge_ratio` function may look something like this:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccccccc}\n",
    "\\hline\n",
    "\\textbf{stock1} & \\textbf{stock2} & \\textbf{hedge\\_ratio} & \\textbf{distance} & \\textbf{adf\\_stat} & \\textbf{adf\\_pvalue} & \\textbf{is\\_cointegrated}\\\\\n",
    "\\hline\n",
    "A & B & 1.00 & 0.12 & -3.56 & 0.01 & True \\\\\n",
    "C & D & 1.00 & 0.23 & -3.14 & 0.02 & True \\\\\n",
    "E & F & 1.00 & 0.40 & -2.26 & 0.04 & True \\\\\n",
    "E & G & 1.00 & 0.56 & -2.75 & 0.03 & True \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "For extra credit (0.5 points): in addition to the OLS and unit hedge ratio methods, implement your method for computing a hedge ratio, which is different from OLS and produces different values. Don't just choose a single integer, such as 2. Then, in your code, choose which regression method to use based on the `HEDGE_RATIO_METHOD` configuration parameter of the `config` argument, which you can set to a string such as `\"ols\"`, `\"unit\"`, or `\"my_method\"` (please choose a more descriptive name than \"my_method\").\n",
    "\n",
    "Our implementation is fairly naive and constitutes about 20 lines. You can do it in fewer or more lines.\n",
    "\n",
    "Complete the function in `backtest.py` and paste your complete `estimate_hedge_ratio` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test your implementation. Run the cell below. The spread you observe should appear to be mostly oscillating around its mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the pairs you created above and prices and returns and\n",
    "# pass them to the `estimate_hedge_ratio` function.\n",
    "\n",
    "config = {\n",
    "    'DISTANCE_METRIC': 'ssd',\n",
    "    'HEDGE_RATIO_METHOD': 'unit',\n",
    "    'COINT_THRESHOLD': 0.05,\n",
    "}\n",
    "\n",
    "# Estimate hedge ratios\n",
    "hedge_ratios = estimate_hedge_ratio(filtered_prices, pairs, config)\n",
    "display(hedge_ratios)\n",
    "\n",
    "# Plot the first hedge ratio as a spread\n",
    "pair_idx = 0\n",
    "\n",
    "ticker1 = hedge_ratios['stock1'].iloc[pair_idx]\n",
    "ticker2 = hedge_ratios['stock2'].iloc[pair_idx]\n",
    "hr = hedge_ratios['hedge_ratio'].iloc[pair_idx]\n",
    "s1 = filtered_prices[ticker1]\n",
    "s2 = filtered_prices[ticker2]\n",
    "spread = s1 - hr * s2\n",
    "spread.plot()\n",
    "plt.title(f'Spread: {ticker1} - {hr:.2f} * {ticker2}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified cointegrated pairs and estimated their hedge ratios, we need to generate trading signals based on the divergence and convergence of these pairs. This is a critical component of the Gatev et al. pairs trading strategy.\n",
    "\n",
    "We've created a function called `compute_signal` in `backtest.py`. Your task is to implement this function according to the Gatev et al. methodology. Complete the following steps:\n",
    "\n",
    "1. Loop through each pair in the `hedge_ratios` dataframe.\n",
    "\n",
    "2. Extract the price series for both stocks in each pair from the `prices` dataframe.\n",
    "\n",
    "3. Calculate the spread between the two stocks using the hedge ratio. The spread should be calculated as: `spread = price2 - hedge_ratio * price1`\n",
    "\n",
    "4. Implement a rolling window calculation to standardize the spread:\n",
    "   - Calculate the rolling mean of the spread using a window size of `config['ESTIMATION_PERIOD']`\n",
    "   - Calculate the rolling standard deviation using the same window size\n",
    "   - Compute the z-score: `z_score = (spread - rolling_mean) / rolling_std`\n",
    "\n",
    "5. Initialize an empty signal dataframe for the pair.\n",
    "\n",
    "6. Loop through time for the pair, generating trading signals based on the z-score and price convergence:\n",
    "   - **Entry Conditions**:\n",
    "     - If z-score < -`config['Z_THRESHOLD']`, enter a long spread position (signal = 1)\n",
    "     - If z-score > `config['Z_THRESHOLD']`, enter a short spread position (signal = -1)\n",
    "   \n",
    "   - **Exit Conditions**:\n",
    "     - According to Gatev et al., positions should be closed when the normalized prices cross (converge)\n",
    "     - You need to track which normalized price was higher/lower at trade entry\n",
    "     - Close the position (signal = 0) when the normalized prices cross back over each other\n",
    "\n",
    "7. For each pair, store the following in the signals dictionary:\n",
    "   - stock1 and stock2 identifiers\n",
    "   - hedge_ratio\n",
    "   - signal series\n",
    "   - z_score series\n",
    "   - any additional metadata you find useful e.g. for debugging, such as series for the spread, crosses, mean, volatility, etc.\n",
    "\n",
    "8. Return the completed signals dictionary.\n",
    "\n",
    "## Important Implementation Details\n",
    "\n",
    "- You must properly track the state of each trade (whether you're in a trade or not)\n",
    "- You need to record which normalized price was higher/lower at trade entry to determine when price convergence occurs\n",
    "- The signals should be a time series with the same index as the price data\n",
    "- For each date, the signal should be:\n",
    "  - 1: Long spread (short stock1, long stock2)\n",
    "  - -1: Short spread (long stock1, short stock2)\n",
    "  - 0: No position\n",
    "\n",
    "## Example Output\n",
    "\n",
    "The returned dictionary should have keys for each pair and values containing the signal information:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'STOCK1_STOCK2': {\n",
    "        'stock1': 'STOCK1',\n",
    "        'stock2': 'STOCK2',\n",
    "        'hedge_ratio': 1.25,\n",
    "        'signal': pd.Series(\n",
    "            data=[0, 0, 1, 1, 0, -1, -1, 0, ...],\n",
    "            index=pd.DatetimeIndex([\n",
    "                '2009-01-01', '2009-01-02', '2009-01-03',\n",
    "                '2009-01-04', '2009-01-05', '2009-01-08',\n",
    "                '2009-01-09', '2009-01-10', ...\n",
    "            ])\n",
    "        ),\n",
    "\n",
    "        'z_score': pd.Series(\n",
    "            data=[0.5, 1.8, 2.2, 1.9, 0.8, -1.2, -2.1, -1.8, ...],\n",
    "            index=pd.DatetimeIndex([\n",
    "                '2009-01-01', '2009-01-02', '2009-01-03',\n",
    "                '2009-01-04', '2009-01-05', '2009-01-08',\n",
    "                '2009-01-09', '2009-01-10', ...\n",
    "            ])\n",
    "        ),\n",
    "        # Add any additional metadata you find useful\n",
    "    },\n",
    "    # Additional pairs...\n",
    "}\n",
    "```\n",
    "\n",
    "<!-- For extra credit (1 points): Implement an alternative exit strategy in addition to the price convergence method from Gatev et al. Add a configuration parameter `EXIT_STRATEGY` that allows switching between:\n",
    "\n",
    "- \"convergence\" - Exit when prices cross (default, as in Gatev et al.)\n",
    "- \"z_score\" - Exit when the z-score returns to zero (or within a small threshold) -->\n",
    "\n",
    "<!-- Don't do this extra credit above, it's commented out ;) -->\n",
    "\n",
    "Our implementation is about 55 lines, but this can be done in more or fewer lines.\n",
    "\n",
    "Complete the function in `backtest.py` and paste the complete `compute_signal` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test your implementation. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the pairs you created above and prices and returns and\n",
    "# pass them to the `estimate_hedge_ratio` function.\n",
    "\n",
    "config = {\n",
    "    'Z_THRESHOLD': 2,\n",
    "    'ESTIMATION_PERIOD': 20,\n",
    "    'EXIT_STRATEGY': 'convergence',\n",
    "}\n",
    "\n",
    "# Compute signals\n",
    "signals = compute_signal(filtered_prices, hedge_ratios, config)\n",
    "pair_idx = 0\n",
    "\n",
    "# Get ticker info for the selected pair\n",
    "ticker1 = hedge_ratios['stock1'].iloc[pair_idx]\n",
    "ticker2 = hedge_ratios['stock2'].iloc[pair_idx]\n",
    "hr = hedge_ratios['hedge_ratio'].iloc[pair_idx]\n",
    "\n",
    "# Create figure with two subplots sharing x-axis\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot normalized prices on first subplot\n",
    "s1 = filtered_prices[ticker1]\n",
    "s2 = filtered_prices[ticker2]\n",
    "s1_norm = s1 / s1.iloc[0]\n",
    "s2_norm = s2 / s2.iloc[0]\n",
    "ax1.plot(s1_norm.index, s1_norm, label=ticker1)\n",
    "ax1.plot(s2_norm.index, s2_norm, label=ticker2)\n",
    "ax1.set_title('Normalized Price')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot spread and signals on second subplot\n",
    "spread = s1 - hr * s2\n",
    "ax2.plot(spread.index, spread, label='Spread')\n",
    "\n",
    "# Add signals to the second subplot\n",
    "pair_signals = signals[f\"{ticker1}_{ticker2}\"]\n",
    "entry_long = np.where(pair_signals['signal'] == -1)[0]\n",
    "entry_short = np.where(pair_signals['signal'] == 1)[0]\n",
    "exit_points = np.where(pair_signals['signal'] == 0)[0]\n",
    "\n",
    "ax2.scatter(spread.index[entry_long], spread.iloc[entry_long], \n",
    "            color='green', marker='^', s=100, label='Long Position')\n",
    "ax2.scatter(spread.index[entry_short], spread.iloc[entry_short], \n",
    "            color='red', marker='v', s=100, label='Short Position')\n",
    "ax2.scatter(spread.index[exit_points], spread.iloc[exit_points], \n",
    "            color='black', marker='o', s=50, label='Flat Position')\n",
    "ax2.set_title(f'Spread and Signals: {ticker1} - {hr:.2f} * {ticker2}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step of the implementation! Now we're going to convert our set of signals for each pair into positions using the `allocate_positions` function. This function will take a set of signals for each pair and allocate the available capital to each pair based on the signals for the given date. The function will then return a set of positions for each stock for the given date. We definitely don't claim this to be the best way to allocate among the pairs, but it's a simple baseline. \n",
    "\n",
    "Let's get into it:\n",
    "\n",
    "1. Initialize an empty dictionary to store positions for each stock.\n",
    "\n",
    "2. Add a few checks to handle edge cases:\n",
    "   - If the current date is not in the price data, return empty positions\n",
    "   - If portfolio cash is None or zero/negative, return empty positions\n",
    "\n",
    "3. Collect active pairs for the current date:\n",
    "   - Iterate through the signals dictionary\n",
    "   - For each pair, extract stock1, stock2, and the hedge ratio\n",
    "   - Skip pairs where we don't have price data for both stocks\n",
    "   - Get the signal with a one-day lag (yesterday's signal for today's execution)\n",
    "   - Skip signals that aren't for the current date or are zero/NaN\n",
    "   - Store active pairs with their relevant information (stocks, hedge ratio, signal value, current prices)\n",
    "\n",
    "4. If no active pairs are found, return empty positions.\n",
    "\n",
    "5. Allocate capital equally across all active pairs:\n",
    "   - Calculate `capital_per_pair = (portfolio_cash * config['MAX_LEVERAGE']) / number of active pairs`\n",
    "\n",
    "6. For each active pair:\n",
    "   - Initialize positions for both stocks if they don't exist in the positions dictionary\n",
    "   - Calculate balanced position sizes using a notional ratio approach:\n",
    "     - `notional_ratio = (hedge_ratio * price2) / price1`\n",
    "     - `shares1 = capital_per_pair / (price1 * (1 + notional_ratio))`\n",
    "     - `shares2 = shares1 * hedge_ratio`\n",
    "   - Update positions based on signal:\n",
    "     - `positions[stock1] += -signal_value * shares1`\n",
    "     - `positions[stock2] += signal_value * shares2`\n",
    "\n",
    "7. Return the positions dictionary.\n",
    "\n",
    "The `allocate_positions` function takes the following parameters:\n",
    "\n",
    "- `signals`: A dictionary with signal information for each pair. Note that this contains signals for the entire trading period, not just the current date. Make sure to avoid using signals from the future!\n",
    "- `config`: Configuration parameters, containing MAX_LEVERAGE\n",
    "- `prices`: DataFrame with stock prices for the entire trading period (dates as index, tickers as columns)\n",
    "- `date`: Current date for price and signal reference\n",
    "- `portfolio_cash`: Available cash in the portfolio\n",
    "\n",
    "It returns a dictionary mapping stock names to position sizes (number of shares).\n",
    "\n",
    "Our implementation is fairly naive and requires about 40 lines of code. You can implement it in fewer or more lines.\n",
    "\n",
    "Complete the function in `backtest.py` and paste the complete `allocate_positions` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def allocate_positions(\n",
    "    signals: dict,\n",
    "    config: dict,\n",
    "    prices: pd.DataFrame,\n",
    "    date: pd.Timestamp,\n",
    "    portfolio_cash: float,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Simple position sizing that allocates cash equally across all active pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signals: dict\n",
    "        Dictionary with signal information for each pair\n",
    "    config: dict\n",
    "        Configuration parameters, containing MAX_LEVERAGE\n",
    "    prices: pd.DataFrame\n",
    "        Dataframe of stock prices with dates as index and tickers as columns\n",
    "    date: pd.Timestamp\n",
    "        Current date for price reference\n",
    "    portfolio_cash: float\n",
    "        Available cash in the portfolio\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with position information for each stock\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # 1. Initialize an empty dictionary to store positions for each stock.\n",
    "    positions = {}\n",
    "\n",
    "    # 2. Add a few checks to handle edge cases:\n",
    "    # If the current date is not in the price data, return empty positions\n",
    "    # If portfolio cash is None or zero/negative, return empty positions\n",
    "    if date not in prices.index:\n",
    "        return positions\n",
    "    if portfolio_cash is None or portfolio_cash <= 0:\n",
    "        return positions\n",
    "\n",
    "    # 3. Collect active pairs for the current date:\n",
    "    # Iterate through the signals dictionary\n",
    "    # For each pair, extract stock1, stock2, and the hedge ratio\n",
    "    # Skip pairs where we don't have price data for both stocks\n",
    "    # Get the signal with a one-day lag (yesterday's signal for today's execution)\n",
    "    # Skip signals that aren't for the current date or are zero/NaN\n",
    "    # Store active pairs with their relevant information (stocks, hedge ratio, signal value, current prices)\n",
    "    active_pairs = {}\n",
    "    for key, val in signals.items(): \n",
    "        stock1, stock2, hedge_ratio = val['stock1'], val['stock2'], val['hedge_ratio']\n",
    "        if stock1 not in prices.columns or stock2 not in prices.columns:\n",
    "            continue\n",
    "        signal_series = val['signal']\n",
    "        lagged_signals = signal_series.shift(1)\n",
    "        if date not in lagged_signals.index:\n",
    "            continue\n",
    "        signal_value = lagged_signals.loc[date]\n",
    "        if pd.isna(signal_value) or signal_value == 0:\n",
    "            continue\n",
    "        \n",
    "        active_pairs[key] = {\n",
    "            'stock1': stock1, \n",
    "            'stock2': stock2,\n",
    "            'hedge_ratio': hedge_ratio,\n",
    "            'signal': signal_value,\n",
    "            'price1': prices.loc[date, stock1],\n",
    "            'price2': prices.loc[date, stock2],\n",
    "        }\n",
    "    \n",
    "    # 4. If no active pairs are found, return empty positions.\n",
    "    if len(active_pairs) < 1:\n",
    "        return positions\n",
    "\n",
    "    # 5. Allocate capital equally across all active pairs:\n",
    "    # Calculate capital_per_pair = (portfolio_cash * config['MAX_LEVERAGE']) / number of active pairs\n",
    "    capital_per_pair = (portfolio_cash * config['MAX_LEVERAGE']) / len(active_pairs)\n",
    "\n",
    "    # 6. For each active pair:\n",
    "    # Initialize positions for both stocks if they don't exist in the positions dictionary\n",
    "    # Calculate balanced position sizes using a notional ratio approach:\n",
    "    #     notional_ratio = (hedge_ratio * price2) / price1\n",
    "    #     shares1 = capital_per_pair / (price1 * (1 + notional_ratio))\n",
    "    #     shares2 = shares1 * hedge_ratio\n",
    "    # Update positions based on signal:\n",
    "    #     positions[stock1] += -signal_value * shares1\n",
    "    #     positions[stock2] += signal_value * shares2\n",
    "    for val in active_pairs.values():\n",
    "        s1, s2, hedge_ratio, signal_val, p1, p2 = val['stock1'], val['stock2'], val['hedge_ratio'], val['signal'], val['price1'], val['price2']\n",
    "\n",
    "        notional_ratio = (hedge_ratio * p2) / p1\n",
    "        shares1 = capital_per_pair / (p1 * (1 + notional_ratio))\n",
    "        shares2 = shares1 * hedge_ratio\n",
    "\n",
    "        positions[s1] = positions.get(s1, 0) + (-signal_val * shares1)\n",
    "        positions[s2] = positions.get(s2, 0) + (signal_val * shares2)\n",
    "    \n",
    "    # 7. Return the positions dictionary\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll test your implementation. Run the following cell and observe the positions. They should appear to follow most of the signals, more or less. Recall that any single stock may be involved in multiple pairs, and so the positions you observe here may not be perfectly correlated with the signals, and may not be perfectly offset at all times. This is expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'MAX_LEVERAGE': 0.05,\n",
    "}\n",
    "\n",
    "pair_idx = 0\n",
    "\n",
    "# Get ticker info for the selected pair\n",
    "ticker1 = hedge_ratios['stock1'].iloc[pair_idx]\n",
    "ticker2 = hedge_ratios['stock2'].iloc[pair_idx]\n",
    "hr = hedge_ratios['hedge_ratio'].iloc[pair_idx]\n",
    "\n",
    "# Compute the positions using allocate_positions\n",
    "positions_t1 = {}\n",
    "positions_t2 = {}\n",
    "for date in filtered_prices.index:\n",
    "    positions_t1[date] = allocate_positions(\n",
    "        signals, config, filtered_prices, pd.Timestamp(date), 1_000_000\n",
    "    ).get(ticker1, 0)\n",
    "    positions_t2[date] = allocate_positions(\n",
    "        signals, config, filtered_prices, pd.Timestamp(date), 1_000_000\n",
    "    ).get(ticker2, 0)\n",
    "\n",
    "positions = pd.DataFrame(\n",
    "    {ticker1: positions_t1.values(), ticker2: positions_t2.values()},\n",
    "    index=filtered_prices.index,\n",
    ")\n",
    "display(positions)\n",
    "\n",
    "# Create figure with three subplots sharing x-axis\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot normalized prices on first subplot\n",
    "s1 = filtered_prices[ticker1]\n",
    "s2 = filtered_prices[ticker2]\n",
    "s1_norm = s1 / s1.iloc[0]\n",
    "s2_norm = s2 / s2.iloc[0]\n",
    "ax1.plot(s1_norm.index, s1_norm, label=ticker1)\n",
    "ax1.plot(s2_norm.index, s2_norm, label=ticker2)\n",
    "ax1.set_title('Normalized Price')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot spread and signals on second subplot\n",
    "spread = s1 - hr * s2\n",
    "ax2.plot(spread.index, spread, label='Spread')\n",
    "\n",
    "# Add signals to the second subplot\n",
    "pair_signals = signals[f\"{ticker1}_{ticker2}\"]\n",
    "entry_long = np.where(pair_signals['signal'] == -1)[0]\n",
    "entry_short = np.where(pair_signals['signal'] == 1)[0]\n",
    "exit_points = np.where(pair_signals['signal'] == 0)[0]\n",
    "\n",
    "ax2.scatter(spread.index[entry_long], spread.iloc[entry_long], \n",
    "            color='green', marker='^', s=100, label='Long Position')\n",
    "ax2.scatter(spread.index[entry_short], spread.iloc[entry_short], \n",
    "            color='red', marker='v', s=100, label='Short Position')\n",
    "ax2.scatter(spread.index[exit_points], spread.iloc[exit_points], \n",
    "            color='black', marker='o', s=50, label='Flat Position')\n",
    "ax2.set_title(f'Spread and Signals: {ticker1} - {hr:.2f} * {ticker2}')\n",
    "ax2.legend()\n",
    "\n",
    "# Now plot the positions on the third subplot\n",
    "ax3.plot(positions.index, positions[ticker1], label=ticker1)\n",
    "ax3.plot(positions.index, positions[ticker2], label=ticker2)\n",
    "ax3.set_title('Positions (Number of Shares)')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll run a backtest. Run the cell below to see the results of the strategy. It should take about 3 minutes. Note that the results will be written to disk after the backtest is complete, so if you run it again with the same `config` settings, it will load the data and produce a performance analysis in just a few seconds. If you want the backtest to re-run instead of load data, make sure to delete the `results` directory it will create.\n",
    "\n",
    "Don't be surprised if the results are poor--this basic style of pairs trading does not typically yield significant returns or high Sharpe ratios anymore. However, we've still learned a lot from it, and (significant) variations of pairs trading are still used in industry. The intuition behind the strategy is still sound. Particular implementations of the idea (such as those widely publicized in academic papers), however, will often lose efficacy over time as other market participants \"trade away\" the particular signals, aka edge, that the strategy exploits. This is known as *alpha decay*.\n",
    "\n",
    "More importantly, don't rush into conclusions about the performance of the strategy if it looks good to you. This is a slightly short backtest over a set of equities which was selected in a way that is affected by lookback bias (recall from HW1 that we only selected stocks which were in existence in one particular year of the Nikkei 225). We've also neglected to model a few important things, such as the price impact and bid-ask spread. And most importantly, we've made a dangerously optimistic assumption: that we have the ability to short all of these assets at any time. Japan has several restrictions on shorting, and a broker may either prevent you from borrowing some of these assets, or may *recall* your borrowed asset (perhaps in the middle of a trade!) if the owner wants to sell it. This is a very significant issue, and can turn a profitable-looking strategy into a losing one in reality. Because we don't have historical data on Japanese shorting restrictions (or proxies for it which Gatev et al. 2006 use, such as volume), we've neglected to model this risk in this assignment. For these reasons, it's likely that today, any positive alpha from this strategy is substantially smaller than it appears to be, and probably negligible--or even negative.\n",
    "\n",
    "All that aside, take a look at the results and briefly comment on what you observe in the test below.  Try to explore or explain why the strategy has performed the way it has. This is an open-ended question, and there is no right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prices, returns, tickers, metadata, portfolio_history, metrics_df, pairs_df) = run_backtest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8 (extra credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(extra credit: 1-5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a relatively new class, we're still developing some of the assignments, and like anyone, we're fallible. If you find a bug in the code, please report it to us, and we'll fix it. For doing so, we'll award you extra credit which scales linearly with the severity of the bug(s). Please describe your bug(s) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9 (ungraded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ungraded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, feel free to experiment with different settings for the configuration dictionary or on the data and report any interesting results you discover. We won't grade this, but we will read it. Feel free to add any additional functionality to `backtest.py` to help you with your experiments. If you modify one of the functions pasted in a prior question, please leave the original code in that cell.\n",
    "\n",
    "If you're still interested in exploring, here are several ideas for improvement:\n",
    "\n",
    "1. To make this course more accessible, this backtester uses vanilla Python code (including Python loops), pandas dataframes, and Python objects. Even worse, it loops over Pandas dataframes, which is very inefficient. It would be a lot faster if we made more efficient choices, such as using NumPy arrays, taking advantage of vectorized functions, or using Cython. Try timing or profiling the `run_backtest` function and see where the bottlenecks are in the code. Then implement these more efficient methods for the one function where the most time is spent. How much does it speed up your code?\n",
    "2. This backtester is written in a way which iterates over pairs formation periods and trading periods. This makes it easy to carve out functions for implementation, but it's not very realistic or extensible to other types of strategies. Moreover, it doesn't exactly correspond to the method of Gatev et al. 2006, which runs six separate portfolios, each of which has its own formation period and trading period, which are offset by one month. In this implementation, we only form new pairs every six months. Try changing the backtester to instead iterate over days, and form new pairs at the beginning or end of every month. \n",
    "3. The `allocate_positions` function we propose is quite naive. Try to implement a smarter `allocate_positions` function which allocates capital more efficiently and controls risk a more carefully when there's a smaller number of pairs to trade. Similarly, you might want to increase risk when there's a larger number of pairs to trade--or not.\n",
    "4. The return calculation is based on the entire amount of initial capital, even though under some position formation policies, such as ours, all of it may not be used at all times. Try implementing two return calculations: one which is based on committed capital, and another which is based on committed and allocated capital, as done in the Gatev et al. 2006 paper. If you're really ambitious, you could go even further and implement margin requirements, leverage limits, and margin calls. Then add a configuration option to enable or disable them to see how they affect the results.\n",
    "5. There are some other robustness tests and metrics which we could add, from both the Gatev et al. 2006 paper and the broader literature. Try adding some of them. Comparing to a benchmark, computing more information about drawdowns, computing tables of the average, best, and worst returns, days, weeks, and months, and more would be welcome. Check out the quantstats package for ideas."
   ]
  }
 ],
 "metadata": {
  "date": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "title": "MS&E 244: Homework 2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
